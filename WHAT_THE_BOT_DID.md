# What Did The Bot Do? - Explained

## ğŸ¤” Your Questions Answered

### 1. **Did it train on the past 3 years of the coin?**
**Answer: NO** - It only trained on **30 days** of data (1-hour candles)

### 2. **Is it supposed to download candle data to Dropbox too?**
**Answer: YES** - But the test script didn't do this. It should upload candle data to Dropbox.

---

## ğŸ“Š What Actually Happened

### Training Data Used
- **Days**: 30 days (default in test script)
- **Timeframe**: 1 hour candles
- **Data Source**: Loaded from local cache (existing parquet files)
- **Total Rows**: ~216,000 rows per coin (30 days Ã— 24 hours Ã— ~300 samples per hour)

### What Got Uploaded to Dropbox
âœ… **Model files** (model.bin) - 3 files  
âœ… **Metrics files** (metrics.json) - 3 files  
âŒ **Candle data** - NOT uploaded (should be uploaded)

---

## ğŸ” Detailed Breakdown

### Step-by-Step What Happened:

1. **Data Loading**:
   - Script looked for cached data in `data/candles/`
   - Found existing parquet files (from previous downloads)
   - Loaded ~216,000 rows of 1-hour candle data
   - **Did NOT download fresh data from exchange**
   - **Did NOT upload data to Dropbox**

2. **Feature Building**:
   - Built 15 features from candle data:
     - Returns (1h, 5h, 20h)
     - Price ratios (high/low, close/open)
     - Moving averages (SMA 20, SMA 50)
     - Volume ratios
     - RSI, volatility
     - Time features (hour, day, weekend)

3. **Model Training**:
   - Split data: 80% train (~172K samples), 20% test (~43K samples)
   - Trained XGBoost model on training set
   - Evaluated on test set
   - Generated metrics (Sharpe, hit rate, RÂ²)

4. **Artifact Creation**:
   - Saved model.bin (trained model)
   - Saved metrics.json (performance stats)
   - Saved config.json (model configuration)
   - Saved sha256.txt (file hash)

5. **Dropbox Upload**:
   - âœ… Uploaded model.bin
   - âœ… Uploaded metrics.json
   - âŒ Did NOT upload candle data
   - âŒ Did NOT upload config.json
   - âŒ Did NOT upload sha256.txt

---

## âŒ What's Missing (What Should Happen)

### 1. **More Training Data**
- **Current**: 30 days
- **Recommended**: 150-365 days (or more for better models)
- **For 3 years**: Use `--days 1095` (3 years Ã— 365 days)

### 2. **Candle Data Upload to Dropbox**
- **Current**: Candle data only stored locally
- **Should**: Upload to Dropbox at `/Runpodhuracan/data/candles/`
- **Purpose**: RunPod engine can restore from Dropbox instead of downloading from exchange

### 3. **Fresh Data Download**
- **Current**: Used cached data (may be old)
- **Should**: Download fresh data from exchange
- **Then**: Upload to Dropbox for future use

---

## âœ… Proper Workflow (What Should Happen)

### Step 1: Download Candle Data
```bash
# Download 3 years of data (1095 days)
python scripts/simple_download_candles.py \
  --symbols BTC/USDT ETH/USDT SOL/USDT \
  --days 1095 \
  --timeframe 1h
```

This will:
- Download fresh data from exchange
- Save to local `data/candles/`
- **Upload to Dropbox** at `/Runpodhuracan/data/candles/`

### Step 2: Train Models
```bash
# Train on 3 years of data
python scripts/test_end_to_end_training.py \
  --symbols BTC/USDT ETH/USDT SOL/USDT \
  --days 1095 \
  --timeframe 1h
```

This will:
- Load data from cache (or download if missing)
- Train models on 3 years of data
- Upload models and metrics to Dropbox

---

## ğŸ”§ Fix: Update Test Script to Upload Candle Data

The test script should also upload candle data to Dropbox. Here's what needs to be added:

### Current Behavior:
1. âœ… Load candle data (from cache)
2. âœ… Train model
3. âœ… Upload model.bin
4. âœ… Upload metrics.json
5. âŒ **Missing**: Upload candle data to Dropbox

### Should Be:
1. âœ… Download candle data (if not in cache)
2. âœ… **Upload candle data to Dropbox** (NEW)
3. âœ… Train model
4. âœ… Upload model.bin
5. âœ… Upload metrics.json
6. âœ… Upload config.json
7. âœ… Upload sha256.txt

---

## ğŸ“ Expected Dropbox Structure

### What You Should See:

```
/Runpodhuracan/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ candles/                    â† Candle data (MISSING)
â”‚       â”œâ”€â”€ BTC/
â”‚       â”‚   â””â”€â”€ BTC-USDT_1h_*.parquet
â”‚       â”œâ”€â”€ ETH/
â”‚       â”‚   â””â”€â”€ ETH-USDT_1h_*.parquet
â”‚       â””â”€â”€ SOL/
â”‚           â””â”€â”€ SOL-USDT_1h_*.parquet
â””â”€â”€ huracan/
    â””â”€â”€ models/
        â””â”€â”€ baselines/
            â””â”€â”€ 20251111/
                â”œâ”€â”€ BTCUSDT/
                â”‚   â”œâ”€â”€ model.bin    â† âœ… Uploaded
                â”‚   â””â”€â”€ metrics.json â† âœ… Uploaded
                â”œâ”€â”€ ETHUSDT/
                â”‚   â”œâ”€â”€ model.bin    â† âœ… Uploaded
                â”‚   â””â”€â”€ metrics.json â† âœ… Uploaded
                â””â”€â”€ SOLUSDT/
                    â”œâ”€â”€ model.bin    â† âœ… Uploaded
                    â””â”€â”€ metrics.json â† âœ… Uploaded
```

### What's Actually There:
- âœ… Models and metrics (in `huracan/models/baselines/`)
- âŒ Candle data (missing from `data/candles/`)

---

## ğŸš€ How to Do It Properly

### Option 1: Use Separate Scripts (Recommended)

**Step 1: Download and Upload Candle Data**
```bash
python scripts/simple_download_candles.py \
  --symbols BTC/USDT ETH/USDT SOL/USDT \
  --days 1095 \
  --timeframe 1h
```

This script:
- Downloads data from exchange
- Saves to local cache
- **Uploads to Dropbox** automatically

**Step 2: Train Models**
```bash
python scripts/test_end_to_end_training.py \
  --symbols BTC/USDT ETH/USDT SOL/USDT \
  --days 1095 \
  --timeframe 1h
```

This script:
- Loads data from cache
- Trains models
- Uploads models and metrics

### Option 2: Enhanced Test Script (Future)

Update `test_end_to_end_training.py` to:
1. Download candle data (if not in cache)
2. Upload candle data to Dropbox
3. Train model
4. Upload all artifacts (model, metrics, config, hash)

---

## ğŸ“Š Data Comparison

### What You Got (30 Days):
- **Training Samples**: ~172K per coin
- **Test Samples**: ~43K per coin
- **Time Range**: Last 30 days
- **Data Quality**: Good, but limited history

### What You Should Get (3 Years):
- **Training Samples**: ~2.1M per coin (3 years Ã— 365 days Ã— 24 hours Ã— 0.8)
- **Test Samples**: ~525K per coin (3 years Ã— 365 days Ã— 24 hours Ã— 0.2)
- **Time Range**: Last 3 years
- **Data Quality**: Much better, captures long-term patterns

### Impact on Model Quality:
- **30 days**: Limited patterns, may overfit to recent trends
- **3 years**: Captures multiple market cycles, better generalization

---

## ğŸ¯ Summary

### What Happened:
1. âœ… Loaded 30 days of cached candle data
2. âœ… Trained XGBoost models
3. âœ… Uploaded models and metrics to Dropbox
4. âŒ Did NOT upload candle data to Dropbox
5. âŒ Only used 30 days (not 3 years)

### What Should Happen:
1. âœ… Download 3 years of fresh candle data
2. âœ… Upload candle data to Dropbox
3. âœ… Train models on 3 years of data
4. âœ… Upload all artifacts (model, metrics, config, hash)

### Next Steps:
1. **Download 3 years of data**:
   ```bash
   python scripts/simple_download_candles.py \
     --symbols BTC/USDT ETH/USDT SOL/USDT \
     --days 1095 \
     --timeframe 1h
   ```

2. **Train on 3 years**:
   ```bash
   python scripts/test_end_to_end_training.py \
     --symbols BTC/USDT ETH/USDT SOL/USDT \
     --days 1095 \
     --timeframe 1h
   ```

3. **Verify Dropbox** has:
   - Candle data in `/Runpodhuracan/data/candles/`
   - Models in `/Runpodhuracan/huracan/models/baselines/`

---

## ğŸ”— Related Files

- Test Script: `scripts/test_end_to_end_training.py`
- Download Script: `scripts/simple_download_candles.py`
- Upload Script: `scripts/download_and_upload_candles.py`
- Data Loader: `src/cloud/training/datasets/data_loader.py`
- Dropbox Sync: `src/cloud/training/integrations/dropbox_sync.py`

