#!/usr/bin/env python3
"""
End-to-End Test: Download, Train, and Upload Model

Complete test run for one coin:
1. Download candle data
2. Train a model
3. Create model artifacts
4. Upload to Dropbox
"""

import argparse
import os
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, Optional

import structlog
import pandas as pd
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.cloud.training.config.settings import EngineSettings
from src.cloud.training.datasets.data_loader import CandleDataLoader, CandleQuery
from src.cloud.training.datasets.quality_checks import DataQualitySuite
from src.cloud.training.services.exchange import ExchangeClient
from src.cloud.training.integrations.dropbox_sync import DropboxSync
from src.shared.contracts.writer import ContractWriter
from src.shared.contracts.per_coin import (
    RunManifest,
    PerCoinMetrics,
    CostModel,
    FeatureRecipe,
)
from src.shared.config_loader import load_config
from src.shared.features.feature_builder import FeatureBuilder, FeatureRecipe as FBRecipe

# Try to import model training dependencies
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("‚ö†Ô∏è  LightGBM not available, will use stub model")

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("‚ö†Ô∏è  XGBoost not available, will use stub model")

logger = structlog.get_logger(__name__)


def download_candle_data(
    symbol: str,
    days: int = 180,
    timeframe: str = "1d",
    exchange_id: str = "binance",
    upload_to_dropbox: bool = True,
    dropbox_token: Optional[str] = None,
) -> pd.DataFrame:
    """Download candle data for a symbol and optionally upload to Dropbox.
    
    Args:
        symbol: Trading symbol (e.g., "BTC/USDT")
        days: Number of days of history
        timeframe: Timeframe (e.g., "1d", "1h")
        exchange_id: Exchange ID
        upload_to_dropbox: Whether to upload to Dropbox
        dropbox_token: Dropbox access token
        
    Returns:
        DataFrame with OHLCV data
    """
    print(f"üì• Downloading {symbol} data...")
    
    settings = EngineSettings.load(environment=os.getenv("HURACAN_ENV", "local"))
    credentials = settings.exchange.credentials.get(exchange_id, {})
    exchange = ExchangeClient(exchange_id, credentials=credentials, sandbox=settings.exchange.sandbox)
    
    # Use very low coverage threshold for testing (or skip validation)
    quality_suite = DataQualitySuite(coverage_threshold=0.01)  # Very low threshold for testing
    loader = CandleDataLoader(
        exchange_client=exchange,
        quality_suite=quality_suite,
        cache_dir=Path("data/candles"),
    )
    
    end_at = datetime.now(tz=timezone.utc)
    start_at = end_at - timedelta(days=days)
    
    query = CandleQuery(
        symbol=symbol,
        timeframe=timeframe,
        start_at=start_at,
        end_at=end_at,
    )
    
    # Try to load from cache first (bypasses validation)
    cache_path = loader._cache_path(query)
    was_downloaded = False
    
    # Also try to find any cached file for this symbol
    import polars as pl
    symbol_clean = symbol.replace("/", "-").replace(":", "-")
    cache_dir = Path("data/candles")
    symbol_cache_dir = cache_dir / symbol_clean.split("-")[0]  # e.g., BTC
    
    cached_files = []
    if symbol_cache_dir.exists():
        cached_files = list(symbol_cache_dir.glob("*.parquet"))
    
    frame = None
    used_cache_path = None
    
    if cached_files:
        # Use the most recent cached file
        latest_cache = max(cached_files, key=lambda p: p.stat().st_mtime)
        print(f"   üì¶ Found cached data: {latest_cache}")
        try:
            frame = pl.read_parquet(latest_cache)
            if not frame.is_empty():
                # Filter to requested date range if possible
                if "ts" in frame.columns:
                    start_ts = start_at.timestamp() * 1000
                    end_ts = end_at.timestamp() * 1000
                    frame_filtered = frame.filter((pl.col("ts") >= start_ts) & (pl.col("ts") <= end_ts))
                    if not frame_filtered.is_empty():
                        frame = frame_filtered
                    # If filtering removes all data, use all cached data
                
                used_cache_path = latest_cache
                print(f"‚úÖ Loaded {len(frame)} rows from cache")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Cache load failed: {e}")
    
    # Try cache path from loader
    if frame is None and cache_path.exists():
        print(f"   üì¶ Loading from cache: {cache_path}")
        try:
            frame = pl.read_parquet(cache_path)
            if not frame.is_empty():
                used_cache_path = cache_path
                print(f"‚úÖ Loaded {len(frame)} rows from cache")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Cache load failed: {e}, will try download")
    
    # Download with validation (skip if we have cached data)
    if frame is None:
        try:
            frame = loader.load(query, use_cache=True)
            if not frame.is_empty():
                was_downloaded = True
                used_cache_path = cache_path
                print(f"‚úÖ Downloaded {len(frame)} rows")
        except ValueError as e:
            if "Coverage" in str(e):
                print(f"   ‚ö†Ô∏è  Validation failed: {e}")
                print(f"   üí° Tip: Use cached data or download with: python scripts/simple_download_candles.py --symbols {symbol} --days {days} --timeframe {timeframe}")
            raise
    
    if frame is None or frame.is_empty():
        raise ValueError(f"No data available for {symbol}. Please download data first using: python scripts/simple_download_candles.py --symbols {symbol} --days {days} --timeframe {timeframe}")
    
    # Upload to Dropbox if requested
    if upload_to_dropbox and used_cache_path:
        try:
            token = dropbox_token or settings.dropbox.access_token or os.getenv("DROPBOX_ACCESS_TOKEN")
            if token:
                dropbox_sync = DropboxSync(
                    access_token=token,
                    app_folder=settings.dropbox.app_folder,
                    enabled=True,
                )
                
                # Get relative path from data/candles/
                rel_path = used_cache_path.relative_to(Path("data/candles"))
                remote_path = f"/{settings.dropbox.app_folder}/data/candles/{rel_path.as_posix()}"
                
                # Upload to Dropbox
                success = dropbox_sync.upload_file(
                    local_path=str(used_cache_path),
                    remote_path=remote_path,
                    use_dated_folder=False,
                    overwrite=True,
                )
                
                if success:
                    print(f"   ‚úÖ Uploaded candle data to Dropbox: {remote_path}")
                else:
                    print(f"   ‚ö†Ô∏è  Failed to upload candle data to Dropbox")
            else:
                print(f"   ‚ö†Ô∏è  No Dropbox token available, skipping candle data upload")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Failed to upload candle data to Dropbox: {e}")
    
    return frame.to_pandas()


def build_features(df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """Build features from candle data.
    
    Args:
        df: DataFrame with OHLCV data
        symbol: Trading symbol
        
    Returns:
        DataFrame with features
    """
    print(f"üîß Building features for {symbol}...")
    
    # Initialize feature builder
    recipe = FBRecipe(
        timeframes=["1d"],
        indicators={
            "rsi": {"window": 14},
            "ema": {"window": 20},
            "volatility": {"window": 20},
            "momentum": {"window": 10},
        },
        fill_rules={"strategy": "forward_fill"},
        normalization={"type": "standard", "scaler": "StandardScaler"},
        window_sizes={"short": 20, "medium": 50, "long": 200},
    )
    recipe.hash = recipe.compute_hash()
    
    builder = FeatureBuilder(recipe=recipe)
    
    # Build features for each row using rolling window
    features_list = []
    min_window = max(recipe.window_sizes.get("long", 200), 50)  # Need enough history
    
    for idx in range(min_window, len(df)):
        # Get window of data up to current row (use last min_window rows)
        window_df = df.iloc[max(0, idx-min_window):idx+1].copy()
        features = builder.build_features(window_df, symbol)
        features_list.append(features)
    
    # Convert to DataFrame
    if features_list:
        features_df = pd.DataFrame(features_list)
        # Align with original DataFrame (skip first min_window rows)
        features_df.index = df.index[min_window:]
    else:
        # Fallback: create simple features if not enough data
        print(f"   ‚ö†Ô∏è  Not enough data for full features, using simple features")
        features_df = pd.DataFrame()
        features_df['ret_1'] = df['close'].pct_change()
        features_df['ret_5'] = df['close'].pct_change(5)
        features_df['ret_20'] = df['close'].pct_change(20)
        features_df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features_df = features_df.dropna()
    
    print(f"‚úÖ Built {len(features_df.columns)} features, {len(features_df)} samples")
    return features_df


def train_model(
    X: pd.DataFrame,
    y: pd.Series,
    model_type: str = "xgboost",
) -> Any:
    """Train a model on features and targets.
    
    Args:
        X: Feature matrix
        y: Target values
        model_type: Model type ("xgboost" or "lightgbm")
        
    Returns:
        Trained model
    """
    print(f"üéØ Training {model_type} model...")
    
    if model_type == "xgboost" and HAS_XGBOOST:
        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1,
        )
        model.fit(X.values, y.values)
        print(f"‚úÖ Model trained: {model_type}")
        return model
    elif model_type == "lightgbm" and HAS_LIGHTGBM:
        model = lgb.LGBMRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1,
        )
        model.fit(X.values, y.values)
        print(f"‚úÖ Model trained: {model_type}")
        return model
    else:
        # Stub model for testing
        print(f"‚ö†Ô∏è  Using stub model (neither XGBoost nor LightGBM available)")
        return {"model_type": model_type, "trained": True}


def calculate_metrics(
    y_true: pd.Series,
    y_pred: pd.Series,
) -> Dict[str, float]:
    """Calculate model metrics.
    
    Args:
        y_true: True values
        y_pred: Predicted values
        
    Returns:
        Dictionary of metrics
    """
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # Calculate Sharpe-like metric (assuming returns)
    returns = y_pred - y_true
    sharpe = returns.mean() / returns.std() if returns.std() > 0 else 0.0
    
    # Calculate hit rate (percentage of correct direction)
    hit_rate = ((y_pred > 0) == (y_true > 0)).mean()
    
    return {
        "mse": mse,
        "mae": mae,
        "r2": r2,
        "sharpe": sharpe,
        "hit_rate": hit_rate,
        "mean_return": returns.mean(),
        "std_return": returns.std(),
    }


def save_model_artifacts(
    model: Any,
    symbol: str,
    features_df: pd.DataFrame,
    metrics: Dict[str, float],
    config: Dict[str, Any],
    output_dir: Path,
) -> Dict[str, Path]:
    """Save model artifacts to disk.
    
    Args:
        model: Trained model
        symbol: Trading symbol
        features_df: Features DataFrame
        metrics: Model metrics
        config: Model configuration
        output_dir: Output directory
        
    Returns:
        Dictionary of artifact paths
    """
    print(f"üíæ Saving model artifacts for {symbol}...")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    artifacts = {}
    
    # Save model
    import pickle
    model_path = output_dir / "model.bin"
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    artifacts["model"] = model_path
    print(f"   ‚úÖ Saved model: {model_path}")
    
    # Save config
    config_path = output_dir / "config.json"
    import json
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    artifacts["config"] = config_path
    print(f"   ‚úÖ Saved config: {config_path}")
    
    # Save metrics
    metrics_path = output_dir / "metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    artifacts["metrics"] = metrics_path
    print(f"   ‚úÖ Saved metrics: {metrics_path}")
    
    # Calculate and save hash
    from src.cloud.training.utils.hash_utils import compute_file_hash, write_hash_file
    model_hash = compute_file_hash(str(model_path))
    if model_hash:
        hash_path = output_dir / "sha256.txt"
        write_hash_file(str(model_path), model_hash, str(hash_path))
        artifacts["hash"] = hash_path
        print(f"   ‚úÖ Saved hash: {hash_path}")
    
    return artifacts


def upload_to_dropbox(
    artifacts: Dict[str, Path],
    symbol: str,
    date_str: str,
    dropbox_token: Optional[str] = None,
) -> Dict[str, str]:
    """Upload model artifacts to Dropbox.
    
    Args:
        artifacts: Dictionary of artifact paths
        symbol: Trading symbol
        date_str: Date string in YYYYMMDD format
        dropbox_token: Dropbox access token
        
    Returns:
        Dictionary of uploaded paths
    """
    print(f"üì§ Uploading artifacts to Dropbox...")
    
    settings = EngineSettings.load(environment=os.getenv("HURACAN_ENV", "local"))
    token = dropbox_token or settings.dropbox.access_token
    
    if not token:
        print("‚ö†Ô∏è  No Dropbox token available, skipping upload")
        return {}
    
    dropbox_sync = DropboxSync(
        access_token=token,
        app_folder=settings.dropbox.app_folder,
        enabled=True,
    )
    
    contract_writer = ContractWriter(
        dropbox_sync=dropbox_sync,
        base_folder="huracan",
    )
    
    uploaded_paths = {}
    
    # Upload model file
    model_path = artifacts.get("model")
    if model_path and model_path.exists():
        remote_path = contract_writer.write_model_file(
            model_path=str(model_path),
            symbol=symbol,
            date_str=date_str,
        )
        if remote_path:
            uploaded_paths["model"] = remote_path
            print(f"   ‚úÖ Uploaded model: {remote_path}")
    
    # Upload metrics
    metrics_path = artifacts.get("metrics")
    if metrics_path and metrics_path.exists():
        import json
        with open(metrics_path, 'r') as f:
            metrics_data = json.load(f)
        
        # Create PerCoinMetrics object
        metrics = PerCoinMetrics(
            symbol=symbol,
            sample_size=metrics_data.get("sample_size", 0),
            gross_pnl_pct=metrics_data.get("gross_pnl_pct", 0.0),
            net_pnl_pct=metrics_data.get("net_pnl_pct", 0.0),
            sharpe=metrics_data.get("sharpe", 0.0),
            hit_rate=metrics_data.get("hit_rate", 0.0),
            max_drawdown_pct=metrics_data.get("max_drawdown_pct", 0.0),
            avg_trade_bps=metrics_data.get("avg_trade_bps", 0.0),
            validation_windows={},
            costs_bps_used={},
        )
        
        # Upload metrics using write_metrics
        try:
            remote_path = contract_writer.write_metrics(metrics, date_str)
            if remote_path:
                uploaded_paths["metrics"] = remote_path
                print(f"   ‚úÖ Uploaded metrics: {remote_path}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Metrics upload failed: {e}")
            # Upload metrics JSON file directly as fallback
            try:
                remote_metrics_path = f"huracan/models/baselines/{date_str}/{symbol}/metrics.json"
                success = dropbox_sync.upload_file(
                    local_path=str(metrics_path),
                    remote_path=f"/{settings.dropbox.app_folder}/{remote_metrics_path}",
                    overwrite=True,
                )
                if success:
                    uploaded_paths["metrics"] = remote_metrics_path
                    print(f"   ‚úÖ Uploaded metrics (fallback): {remote_metrics_path}")
            except Exception as e2:
                print(f"   ‚ö†Ô∏è  Metrics upload fallback also failed: {e2}")
    
    # Upload config
    config_path = artifacts.get("config")
    if config_path and config_path.exists():
        # Config is typically included in the model bundle, but we can upload separately if needed
        pass
    
    # Upload hash
    hash_path = artifacts.get("hash")
    if hash_path and hash_path.exists():
        # Hash is typically included with the model, but we can upload separately if needed
        pass
    
    print(f"‚úÖ Uploaded {len(uploaded_paths)} files to Dropbox")
    return uploaded_paths


def main():
    """Main entry point for end-to-end test."""
    parser = argparse.ArgumentParser(description="End-to-end test: Download, train, and upload model")
    parser.add_argument("--symbol", type=str, help="Trading symbol (e.g., BTC/USDT)")
    parser.add_argument("--symbols", type=str, nargs="+", help="Multiple trading symbols (e.g., BTC/USDT ETH/USDT SOL/USDT)")
    parser.add_argument("--days", type=int, default=1095, help="Number of days of history (default: 1095 = 3 years)")
    parser.add_argument("--timeframe", type=str, default="1h", help="Timeframe (1h, 4h, 1d)")
    parser.add_argument("--model-type", type=str, default="xgboost", choices=["xgboost", "lightgbm"], help="Model type")
    parser.add_argument("--exchange", type=str, default="binance", help="Exchange ID")
    parser.add_argument("--dropbox-token", type=str, help="Dropbox access token")
    parser.add_argument("--dry-run", action="store_true", help="Dry run (don't upload to Dropbox)")
    
    args = parser.parse_args()
    
    # Determine symbols to process
    if args.symbols:
        symbols = args.symbols
    elif args.symbol:
        symbols = [args.symbol]
    else:
        # Default to top 3 coins: BTC, ETH, SOL
        symbols = ["BTC/USDT", "ETH/USDT", "SOL/USDT"]
        print("‚ÑπÔ∏è  No symbols specified, defaulting to top 3: BTC/USDT, ETH/USDT, SOL/USDT")
        print()
    
    # Configure logging (simple setup)
    import logging
    logging.basicConfig(level=logging.INFO)
    structlog.configure(
        processors=[
            structlog.processors.TimeStamper(fmt="iso", utc=True),
            structlog.processors.add_log_level,
            structlog.dev.ConsoleRenderer(),
        ],
        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
    )
    
    date_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    
    print("=" * 70)
    print("üöÄ END-TO-END TRAINING TEST")
    print("=" * 70)
    print(f"Symbols: {', '.join(symbols)}")
    print(f"Days: {args.days}")
    print(f"Timeframe: {args.timeframe}")
    print(f"Model Type: {args.model_type}")
    print(f"Date: {date_str}")
    print(f"Dry Run: {args.dry_run}")
    print("=" * 70)
    print()
    
    # Set Dropbox token as environment variable if provided
    if args.dropbox_token:
        os.environ["DROPBOX_ACCESS_TOKEN"] = args.dropbox_token
        print(f"‚úÖ Dropbox token set from command line")
        print()
    
    # Process each symbol
    results = []
    for idx, symbol in enumerate(symbols, 1):
        print("=" * 70)
        print(f"üìä Processing {symbol} ({idx}/{len(symbols)})")
        print("=" * 70)
        print()
        
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%SZ")
        
        try:
            # Step 1: Download candle data
            print("STEP 1: Downloading candle data")
            print("-" * 70)
            df = download_candle_data(
                symbol=symbol,
                days=args.days,
                timeframe=args.timeframe,
                exchange_id=args.exchange,
                upload_to_dropbox=not args.dry_run,
                dropbox_token=args.dropbox_token,
            )
            print()
            
            # Step 2: Build features
            print("STEP 2: Building features")
            print("-" * 70)
            features_df = build_features(df, symbol)
            print()
            
            # Step 3: Prepare training data
            print("STEP 3: Preparing training data")
            print("-" * 70)
            
            # Create target (next period return) on original dataframe
            df['return'] = df['close'].pct_change().shift(-1)
            
            # Align features with price data (features_df index should match df index)
            # Merge on index
            df_with_features = df.join(features_df, how='inner')
            df_with_features = df_with_features.dropna()
            
            if df_with_features.empty:
                raise ValueError("No valid training data after feature alignment")
            
            # Separate features and target (exclude timestamp and OHLCV columns)
            exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'return', 'ts', 'timestamp']
            # Also exclude any datetime/timestamp columns
            feature_cols = []
            for col in df_with_features.columns:
                if col not in exclude_cols:
                    # Check if column is numeric (not datetime)
                    dtype = df_with_features[col].dtype
                    if pd.api.types.is_numeric_dtype(dtype) and not pd.api.types.is_datetime64_any_dtype(dtype):
                        feature_cols.append(col)
            
            if not feature_cols:
                raise ValueError("No features available after alignment")
            
            X = df_with_features[feature_cols]
            y = df_with_features['return']
            
            # Ensure all feature columns are numeric
            X = X.select_dtypes(include=[np.number])
            if X.empty:
                raise ValueError("No numeric features available")
            
            # Split into train/test (temporal split, not random)
            split_idx = int(len(X) * 0.8)
            X_train = X.iloc[:split_idx]
            X_test = X.iloc[split_idx:]
            y_train = y.iloc[:split_idx]
            y_test = y.iloc[split_idx:]
            
            print(f"   Training samples: {len(X_train)}")
            print(f"   Test samples: {len(X_test)}")
            print(f"   Features: {len(X_train.columns)}")
            print(f"   Feature names: {list(X_train.columns[:5])}..." if len(X_train.columns) > 5 else f"   Feature names: {list(X_train.columns)}")
            print()
            
            # Step 4: Train model
            print("STEP 4: Training model")
            print("-" * 70)
            model = train_model(X_train, y_train, model_type=args.model_type)
            print()
            
            # Step 5: Evaluate model
            print("STEP 5: Evaluating model")
            print("-" * 70)
            
            # Make predictions
            if hasattr(model, 'predict'):
                y_pred_train = pd.Series(model.predict(X_train.values), index=X_train.index)
                y_pred_test = pd.Series(model.predict(X_test.values), index=X_test.index)
            else:
                # Stub predictions
                y_pred_train = y_train * 0.9  # Simple stub
                y_pred_test = y_test * 0.9
            
            # Calculate metrics
            train_metrics = calculate_metrics(y_train, y_pred_train)
            test_metrics = calculate_metrics(y_test, y_pred_test)
            
            print(f"   Train Metrics:")
            print(f"      R¬≤: {train_metrics['r2']:.4f}")
            print(f"      Sharpe: {train_metrics['sharpe']:.4f}")
            print(f"      Hit Rate: {train_metrics['hit_rate']:.4f}")
            print(f"   Test Metrics:")
            print(f"      R¬≤: {test_metrics['r2']:.4f}")
            print(f"      Sharpe: {test_metrics['sharpe']:.4f}")
            print(f"      Hit Rate: {test_metrics['hit_rate']:.4f}")
            print()
            
            # Step 6: Save model artifacts
            print("STEP 6: Saving model artifacts")
            print("-" * 70)
            
            output_dir = Path("models") / symbol.replace("/", "") / timestamp
            output_dir.mkdir(parents=True, exist_ok=True)
            
            config = {
                "symbol": symbol,
                "model_type": args.model_type,
                "features": list(X_train.columns),
                "training_date": datetime.now(timezone.utc).isoformat(),
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "train_metrics": train_metrics,
                "test_metrics": test_metrics,
            }
            
            metrics_for_save = {
                "symbol": symbol,
                "sample_size": len(X_train),
                "sharpe": test_metrics['sharpe'],
                "hit_rate": test_metrics['hit_rate'],
                "net_pnl_pct": test_metrics['mean_return'] * 100,
                "gross_pnl_pct": test_metrics['mean_return'] * 100,
                "max_drawdown_pct": abs(test_metrics['std_return']) * 100,
                "avg_trade_bps": test_metrics['mean_return'] * 10000,
                "status": "ok",
            }
            
            artifacts = save_model_artifacts(
                model=model,
                symbol=symbol,
                features_df=features_df,
                metrics=metrics_for_save,
                config=config,
                output_dir=output_dir,
            )
            print()
            
            # Step 7: Upload to Dropbox
            if not args.dry_run:
                print("STEP 7: Uploading to Dropbox")
                print("-" * 70)
                uploaded_paths = upload_to_dropbox(
                    artifacts=artifacts,
                    symbol=symbol.replace("/", ""),
                    date_str=date_str,
                    dropbox_token=args.dropbox_token,
                )
                print()
            else:
                print("STEP 7: Skipping Dropbox upload (dry-run mode)")
                print("-" * 70)
                uploaded_paths = {}
                print()
            
            # Store results
            result = {
                "symbol": symbol,
                "status": "success",
                "test_r2": test_metrics['r2'],
                "test_sharpe": test_metrics['sharpe'],
                "test_hit_rate": test_metrics['hit_rate'],
                "artifacts_dir": str(output_dir),
                "uploaded_files": len(uploaded_paths),
            }
            results.append(result)
            
            # Summary for this symbol
            print("=" * 70)
            print(f"‚úÖ {symbol} COMPLETED")
            print("=" * 70)
            print(f"Model: {args.model_type}")
            print(f"Test R¬≤: {test_metrics['r2']:.4f}")
            print(f"Test Sharpe: {test_metrics['sharpe']:.4f}")
            print(f"Test Hit Rate: {test_metrics['hit_rate']:.4f}")
            print(f"Artifacts: {output_dir}")
            if uploaded_paths:
                print(f"Uploaded to Dropbox: {len(uploaded_paths)} files")
            print("=" * 70)
            print()
            
        except Exception as e:
            print(f"\n‚ùå ERROR for {symbol}: {e}")
            import traceback
            traceback.print_exc()
            result = {
                "symbol": symbol,
                "status": "failed",
                "error": str(e),
            }
            results.append(result)
            print()
            # Continue with next symbol instead of exiting
            continue
    
    # Final summary
    print("=" * 70)
    print("üìä FINAL SUMMARY")
    print("=" * 70)
    print(f"Total Symbols: {len(symbols)}")
    print(f"Successful: {sum(1 for r in results if r.get('status') == 'success')}")
    print(f"Failed: {sum(1 for r in results if r.get('status') == 'failed')}")
    print()
    
    for result in results:
        if result.get('status') == 'success':
            print(f"‚úÖ {result['symbol']}: R¬≤={result['test_r2']:.4f}, Sharpe={result['test_sharpe']:.4f}, Hit Rate={result['test_hit_rate']:.4f}")
        else:
            print(f"‚ùå {result['symbol']}: {result.get('error', 'Unknown error')}")
    
    print("=" * 70)
    
    # Exit with error if any failed
    if any(r.get('status') == 'failed' for r in results):
        sys.exit(1)


if __name__ == "__main__":
    main()

