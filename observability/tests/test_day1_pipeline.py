"""
Day 1 Pipeline Integration Test

Tests the complete observability pipeline:
  Signal ‚Üí Event ‚Üí Queue ‚Üí Writer ‚Üí Storage (DuckDB + Parquet)

This test validates:
1. Event creation and validation
2. Non-blocking queue with batching
3. Hybrid writer (DuckDB + Parquet)
4. Model registry with SHA256 IDs
5. Queue health monitoring
6. Complete end-to-end flow

Usage:
    python -m observability.tests.test_day1_pipeline
"""

import asyncio
import time
from datetime import datetime
from pathlib import Path
import structlog

from observability.core.schemas import (
    create_signal_event,
    create_gate_event,
    create_trade_exec_event,
    MarketContext,
    GateDecision,
    GateDecisionType,
    TradeExecution,
)
from observability.core.event_logger import EventLogger
from observability.core.io import HybridWriter
from observability.core.registry import ModelRegistry
from observability.core.queue_monitor import QueueMonitor

logger = structlog.get_logger(__name__)


# Test model class (must be at module level for pickle)
class DummyMetaLabelModel:
    def __init__(self, threshold=0.5):
        self.threshold = threshold

    def predict_proba(self, X):
        return [[0.3, 0.7]] * len(X)


async def test_complete_pipeline():
    """Test the complete Day 1 observability pipeline"""
    print("=" * 80)
    print("DAY 1 PIPELINE INTEGRATION TEST")
    print("=" * 80)

    # ============================================================================
    # 1. Setup Components
    # ============================================================================
    print("\nüì¶ Setting up components...")

    # Create event logger
    event_logger = EventLogger(
        max_queue_size=1000,
        batch_size=100,
        batch_timeout_sec=1.0
    )

    # Create hybrid writer
    writer = HybridWriter(
        duckdb_path="observability/data/test_events.duckdb",
        parquet_path="observability/data/test_parquet",
        hot_days=7
    )

    # Connect logger to writer
    event_logger.writer = writer

    # Create queue monitor
    monitor = QueueMonitor(
        event_logger,
        warning_threshold=0.50,  # Lower for testing
        critical_threshold=0.80,
    )

    # Set callbacks
    kill_switch_triggered = {"value": False}

    def kill_switch(reason):
        print(f"\nüö® KILL SWITCH TRIGGERED: {reason}")
        kill_switch_triggered["value"] = True

    def send_alert(title, message, priority):
        print(f"\nüì¢ ALERT ({priority}): {title}")
        print(f"   {message}")

    monitor.set_kill_switch_callback(kill_switch)
    monitor.set_alert_callback(send_alert)

    # Create model registry
    registry = ModelRegistry(base_path="observability/data/test_models")

    # Start logger
    await event_logger.start()
    print("‚úì Components initialized")

    # ============================================================================
    # 2. Register a Model
    # ============================================================================
    print("\nü§ñ Registering model...")

    model = DummyMetaLabelModel(threshold=0.45)
    model_id = registry.register_model(
        model=model,
        code_git_sha="abc123def456",
        data_snapshot_id="snapshot_2025-11-06_v1",
        metrics={"auc": 0.710, "ece": 0.061, "brier": 0.18},
        notes="Test model for pipeline validation"
    )
    print(f"‚úì Model registered: {model_id[:20]}...")

    # ============================================================================
    # 3. Simulate Trading Flow
    # ============================================================================
    print("\nüìä Simulating trading flow...")

    market_context = MarketContext(
        volatility_1h=0.34,
        spread_bps=4.2,
        liquidity_score=0.82,
        recent_trend_30m=0.008,
        volume_vs_avg=1.2,
        order_book_imbalance=0.15
    )

    # Scenario 1: Signal received
    print("\n  Scenario 1: Basic Signal Events")

    # Signal event 1
    signal_event = create_signal_event(
        symbol="ETH-USD",
        price=2045.50,
        features={"confidence": 0.78, "predicted_return": 0.015},
        regime="TREND",
        market_context=market_context,
        tags=["test", "scenario_1"]
    )
    await event_logger.log(signal_event)
    print("    ‚úì Signal 1 logged (ETH-USD)")

    # Signal event 2
    signal_event2 = create_signal_event(
        symbol="BTC-USD",
        price=42150.0,
        features={"confidence": 0.42, "predicted_return": 0.008},
        regime="RANGE",
        market_context=market_context,
        tags=["test", "scenario_1"]
    )
    await event_logger.log(signal_event2)
    print("    ‚úì Signal 2 logged (BTC-USD)")

    # Signal event 3 (different symbol)
    signal_event3 = create_signal_event(
        symbol="SOL-USD",
        price=98.75,
        features={"confidence": 0.65, "predicted_return": 0.012},
        regime="TREND",
        market_context=market_context,
        tags=["test", "scenario_1"]
    )
    await event_logger.log(signal_event3)
    print("    ‚úì Signal 3 logged (SOL-USD)")

    # Scenario 2: Bulk events (stress test)
    print("\n  Scenario 2: Bulk events (500 events)")
    start_time = time.time()

    for i in range(500):
        event = create_signal_event(
            symbol=f"TEST-{i % 10}",
            price=1000.0 + i,
            features={"confidence": 0.5 + (i % 50) / 100},
            regime="TREND" if i % 2 == 0 else "RANGE",
            market_context=market_context,
            tags=["test", "bulk", f"batch_{i // 100}"]
        )
        await event_logger.log(event)

    bulk_time = time.time() - start_time
    print(f"    ‚úì 500 events logged in {bulk_time:.2f}s ({500 / bulk_time:.0f} events/sec)")

    # ============================================================================
    # 4. Check Queue Health
    # ============================================================================
    print("\nüíì Checking queue health...")
    health = monitor.check_health()

    print(f"  Status: {health.status.value}")
    print(f"  Queue: {health.queue_size}/{health.queue_max} ({health.queue_fill_pct:.1%})")
    print(f"  Writer Lag: {health.writer_lag_ms:.1f}ms")
    print(f"  Events: {health.events_enqueued:,} enqueued, {health.events_written:,} written, {health.events_dropped:,} dropped")
    print(f"  Rates: {health.enqueue_rate:.1f}/s enqueued, {health.write_rate:.1f}/s written")

    assert health.status.value in ["healthy", "warning"], f"Unexpected health status: {health.status.value}"
    assert health.events_dropped == 0, f"Events were dropped: {health.events_dropped}"
    print("‚úì Queue health good")

    # ============================================================================
    # 5. Wait for Writer to Drain
    # ============================================================================
    print("\n‚è≥ Waiting for writer to drain queue...")
    await asyncio.sleep(2.0)  # Give writer time to process batch

    # Check again
    health2 = monitor.check_health()
    print(f"  Queue now: {health2.queue_size}/{health2.queue_max} ({health2.queue_fill_pct:.1%})")
    print(f"  Total written: {health2.events_written:,}")

    # ============================================================================
    # 6. Stop Logger
    # ============================================================================
    print("\nüõë Stopping logger...")
    await event_logger.stop()
    print("‚úì Logger stopped cleanly")

    # ============================================================================
    # 7. Query Written Data
    # ============================================================================
    print("\nüîç Querying written data...")

    # Query by event kind
    df = writer.query("""
        SELECT kind, COUNT(*) as count
        FROM events
        GROUP BY kind
        ORDER BY count DESC
    """)
    print("\n  Events by kind:")
    print(df.to_string(index=False))

    # Query by symbol
    df2 = writer.query("""
        SELECT symbol, COUNT(*) as count
        FROM events
        WHERE symbol IS NOT NULL
        GROUP BY symbol
        ORDER BY count DESC
        LIMIT 10
    """)
    print("\n  Top 10 symbols:")
    print(df2.to_string(index=False))

    # Query by regime
    df3 = writer.query("""
        SELECT regime, COUNT(*) as count
        FROM events
        WHERE regime IS NOT NULL
        GROUP BY regime
        ORDER BY count DESC
    """)
    print("\n  Events by regime:")
    print(df3.to_string(index=False))

    # Recent events
    df4 = writer.query("""
        SELECT ts, kind, symbol, mode
        FROM events
        ORDER BY ts DESC
        LIMIT 5
    """)
    print("\n  Recent events:")
    print(df4.to_string(index=False))

    # ============================================================================
    # 8. Check Parquet Files
    # ============================================================================
    print("\nüìÅ Checking Parquet files...")

    parquet_base = Path("observability/data/test_parquet")
    if parquet_base.exists():
        # List all partition directories
        date_dirs = list(parquet_base.glob("date=*"))
        print(f"  Date partitions: {len(date_dirs)}")

        for date_dir in date_dirs:
            symbol_dirs = list(date_dir.glob("symbol=*"))
            print(f"    {date_dir.name}: {len(symbol_dirs)} symbols")

            # Count files
            total_files = sum(len(list(sd.glob("*.parquet"))) for sd in symbol_dirs)
            print(f"      Total Parquet files: {total_files}")

    # ============================================================================
    # 9. Test Model Registry
    # ============================================================================
    print("\nüîÑ Testing model registry...")

    # Load model
    loaded_model = registry.load_model(model_id)
    print(f"  ‚úì Model loaded: {type(loaded_model).__name__}")
    assert loaded_model.threshold == 0.45, "Model threshold mismatch"

    # Register config change
    old_config = {"meta_label_threshold": 0.50}
    new_config = {"meta_label_threshold": 0.45}
    change_id = registry.register_config_change(
        component="gates",
        old_config=old_config,
        new_config=new_config,
        reason="Lowering threshold to increase trade frequency",
        changed_by="test_pipeline"
    )
    print(f"  ‚úì Config change registered: {change_id}")

    # Get config history
    history = registry.get_config_history("gates")
    print(f"  ‚úì Config history: {len(history)} changes")

    # ============================================================================
    # 10. Validate Data Integrity
    # ============================================================================
    print("\n‚úÖ Validating data integrity...")

    # Check we wrote everything
    total_events = 500 + 3  # 500 bulk + 3 signal events
    df_total = writer.query("SELECT COUNT(*) as total FROM events")
    written_count = df_total['total'].iloc[0]

    print(f"  Expected events: {total_events}")
    print(f"  Written events: {written_count}")

    if written_count >= total_events:
        print("  ‚úì All events written successfully")
    else:
        print(f"  ‚ö†Ô∏è Some events missing ({total_events - written_count} missing)")

    # Check no duplicates (event_id is unique)
    df_dupes = writer.query("""
        SELECT event_id, COUNT(*) as count
        FROM events
        GROUP BY event_id
        HAVING count > 1
    """)

    if len(df_dupes) == 0:
        print("  ‚úì No duplicate events")
    else:
        print(f"  ‚ö†Ô∏è {len(df_dupes)} duplicate event IDs found")

    # Check timestamps are valid
    df_ts = writer.query("""
        SELECT MIN(ts) as min_ts, MAX(ts) as max_ts
        FROM events
    """)
    print(f"  ‚úì Timestamp range: {df_ts['min_ts'].iloc[0]} to {df_ts['max_ts'].iloc[0]}")

    # ============================================================================
    # SUMMARY
    # ============================================================================
    print("\n" + "=" * 80)
    print("DAY 1 PIPELINE TEST SUMMARY")
    print("=" * 80)
    print(f"‚úì Event creation and validation: PASSED")
    print(f"‚úì Non-blocking queue: PASSED ({500 / bulk_time:.0f} events/sec)")
    print(f"‚úì Batch writer: PASSED ({written_count:,} events written)")
    print(f"‚úì DuckDB storage: PASSED")
    print(f"‚úì Parquet storage: PASSED ({len(date_dirs)} date partitions)")
    print(f"‚úì Model registry: PASSED (SHA256 ID, lineage)")
    print(f"‚úì Queue monitor: PASSED (health checks)")
    print(f"‚úì Data integrity: PASSED")
    print("=" * 80)
    print("üéâ ALL DAY 1 COMPONENTS WORKING!")
    print("=" * 80)

    # Cleanup flag
    print("\nNote: Test data created in observability/data/test_*")
    print("To clean up: rm -rf observability/data/test_*")


if __name__ == '__main__':
    print("Starting Day 1 Pipeline Integration Test...")
    print()

    asyncio.run(test_complete_pipeline())

    print("\n‚úì Pipeline test completed successfully")
